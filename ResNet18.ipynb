{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**"
      ],
      "metadata": {
        "id": "kp77jU9pfi7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT-5TCceT-HU",
        "outputId": "2e9ebeca-ad2c-407a-9768-4d5aa373349f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!fusermount -u /content/drive\n",
        "!rm -rf /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Project Directory**"
      ],
      "metadata": {
        "id": "NHBAAXY-ftuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "print(f\"Project directory set at: {project_dir}\")"
      ],
      "metadata": {
        "id": "FrZtR8yHUlKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5e1681-6ca1-4e48-beb8-c9c6b1aed1ef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project directory set at: /content/drive/MyDrive/MIA_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install & Upgrade Required Packages**"
      ],
      "metadata": {
        "id": "V4LHMviSf7qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install --upgrade torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "ktTktR2IVTQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc0a598-f605-43ea-b147-264fd07da254",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medmnist)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medmnist)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medmnist)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medmnist)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medmnist)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medmnist)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=192c08eefdbfcbb0664caf31b08a970220a9b9c26fa75c3b28b4cd745d14c66d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m777.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download, Move and List MedMNIST Datasets**"
      ],
      "metadata": {
        "id": "5j1FQaQBgEA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Temporary storage in Colab\n",
        "temp_dataset_path = \"/content/dataset\"\n",
        "\n",
        "# Google Drive destination\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(temp_dataset_path, exist_ok=True)\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "\n",
        "# List of all 4 datasets\n",
        "DATASETS = [\n",
        "    \"octmnist\",\"pneumoniamnist\", \"breastmnist\", \"retinamnist\"\n",
        "]\n",
        "\n",
        "# Download and save all datasets in temp storage\n",
        "for dataset_name in DATASETS:\n",
        "    dataset_class = getattr(medmnist, INFO[dataset_name][\"python_class\"])\n",
        "\n",
        "    # Download train, val splits\n",
        "    dataset_class(split=\"train\", download=True, root=temp_dataset_path)\n",
        "    dataset_class(split=\"val\", download=True, root=temp_dataset_path)\n",
        "\n",
        "    print(f\"{dataset_name} downloaded in temp storage at {temp_dataset_path}\")\n",
        "\n",
        "# Move datasets from temp storage to Google Drive\n",
        "for file in os.listdir(temp_dataset_path):\n",
        "    shutil.move(os.path.join(temp_dataset_path, file), dataset_path)\n",
        "\n",
        "print(f\"All datasets moved to {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sLgpxPvQrh8B",
        "outputId": "0f4e49af-ccb5-4467-9ad3-6e3a49b971dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54.9M/54.9M [00:04<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "octmnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.17M/4.17M [00:01<00:00, 3.64MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pneumoniamnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 560k/560k [00:00<00:00, 706kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breastmnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.29M/3.29M [00:01<00:00, 2.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retinamnist downloaded in temp storage at /content/dataset\n",
            "All datasets moved to /content/drive/MyDrive/MIA_Project/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/MIA_Project/dataset\"\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KdFjN3TdWXvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7632c1d-2aa7-438f-8eee-9f5e4ed3f97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 2 root root 4.0K Apr  6 10:36 dataset\n",
            "drwx------ 6 root root 4.0K Apr  6 10:33 drive\n",
            "drwxr-xr-x 1 root root 4.0K Apr  3 13:37 sample_data\n",
            "['pneumoniamnist.npz', 'retinamnist.npz', 'octmnist.npz', 'breastmnist.npz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Information**"
      ],
      "metadata": {
        "id": "4UlTy92eNsLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import numpy as np\n",
        "\n",
        "# Define datasets\n",
        "DATASETS = [\"octmnist\", \"pneumoniamnist\", \"breastmnist\", \"retinamnist\"]\n",
        "\n",
        "# Function to load and inspect dataset\n",
        "def inspect_dataset(dataset_name):\n",
        "    dataset_class = getattr(medmnist, INFO[dataset_name][\"python_class\"])\n",
        "    dataset_train = dataset_class(split=\"train\", root=dataset_path, download=False)\n",
        "    dataset_val = dataset_class(split=\"val\", root=dataset_path, download=False)\n",
        "\n",
        "    print(f\"\\nDataset: {dataset_name}\")\n",
        "    print(f\"Train set size: {len(dataset_train)}\")\n",
        "    print(f\"Validation set size: {len(dataset_val)}\")\n",
        "    # Convert the PIL Image to a NumPy array using np.array\n",
        "    print(f\"Image shape: {np.array(dataset_train[0][0]).shape}\")\n",
        "    print(f\"Number of classes: {len(INFO[dataset_name]['label'])}\")\n",
        "    print(f\"classes: {INFO[dataset_name]['label']}\")\n",
        "\n",
        "# Inspect each dataset\n",
        "for dataset in DATASETS:\n",
        "    inspect_dataset(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HOKmstMqNwUk",
        "outputId": "ed249475-4c6b-4735-ad38-6538bb8c102a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: octmnist\n",
            "Train set size: 97477\n",
            "Validation set size: 10832\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 4\n",
            "classes: {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}\n",
            "\n",
            "Dataset: pneumoniamnist\n",
            "Train set size: 4708\n",
            "Validation set size: 524\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 2\n",
            "classes: {'0': 'normal', '1': 'pneumonia'}\n",
            "\n",
            "Dataset: breastmnist\n",
            "Train set size: 546\n",
            "Validation set size: 78\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 2\n",
            "classes: {'0': 'malignant', '1': 'normal, benign'}\n",
            "\n",
            "Dataset: retinamnist\n",
            "Train set size: 1080\n",
            "Validation set size: 120\n",
            "Image shape: (28, 28, 3)\n",
            "Number of classes: 5\n",
            "classes: {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combine Datasets**"
      ],
      "metadata": {
        "id": "hOJR0vzyOiCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Path where datasets are saved\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Global label mapping\n",
        "global_label_mapping = {\n",
        "    0: \"choroidal neovascularization\",\n",
        "    1: \"diabetic macular edema\",\n",
        "    2: \"drusen\",\n",
        "    3: \"normal (OCT)\",\n",
        "    4: \"normal (Pneumonia)\",\n",
        "    5: \"pneumonia\",\n",
        "    6: \"malignant\",\n",
        "    7: \"normal, benign (Breast)\",\n",
        "    8: \"Retina 0\",\n",
        "    9: \"Retina 1\",\n",
        "    10: \"Retina 2\",\n",
        "    11: \"Retina 3\",\n",
        "    12: \"Retina 4\"\n",
        "}\n",
        "\n",
        "# List of datasets with label offsets\n",
        "DATASETS = {\n",
        "    \"octmnist\": (4, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (2, [4, 5]),\n",
        "    \"breastmnist\": (2, [6, 7]),\n",
        "    \"retinamnist\": (5, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "# Storage for combined images and labels\n",
        "train_images_list, train_labels_list = [], []\n",
        "val_images_list, val_labels_list = [], []\n",
        "\n",
        "# Load and process each dataset\n",
        "for dataset_name, (num_classes, label_offsets) in DATASETS.items():\n",
        "    dataset_file = os.path.join(dataset_path, f\"{dataset_name}.npz\")\n",
        "\n",
        "    # Load .npz file\n",
        "    with np.load(dataset_file, allow_pickle=True) as data:\n",
        "        train_images = data[\"train_images\"]\n",
        "        train_labels = data[\"train_labels\"]\n",
        "        val_images = data[\"val_images\"]\n",
        "        val_labels = data[\"val_labels\"]\n",
        "\n",
        "    # Convert to grayscale if necessary\n",
        "    if train_images.ndim == 4 and train_images.shape[-1] == 3:  # RGB -> Grayscale\n",
        "        train_images = np.mean(train_images, axis=-1)\n",
        "        val_images = np.mean(val_images, axis=-1)\n",
        "\n",
        "    # Normalize & Store Label Index\n",
        "    for img, label in zip(train_images, train_labels):\n",
        "        img_pil = Image.fromarray(img.astype(np.uint8))  # Convert NumPy array to PIL Image\n",
        "        img_tensor = transform(img_pil)  # Apply transform to PIL Image\n",
        "        class_index = label_offsets[int(label[0])]  # Get class index\n",
        "        train_images_list.append(img_tensor.squeeze(0).numpy())  # Remove singleton dim\n",
        "        train_labels_list.append(class_index)\n",
        "\n",
        "    for img, label in zip(val_images, val_labels):\n",
        "        img_pil = Image.fromarray(img.astype(np.uint8))  # Convert NumPy array to PIL Image\n",
        "        img_tensor = transform(img_pil)  # Apply transform to PIL Image\n",
        "        class_index = label_offsets[int(label[0])]  # Get class index\n",
        "        val_images_list.append(img_tensor.squeeze(0).numpy())\n",
        "        val_labels_list.append(class_index)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "train_images_array = np.array(train_images_list)\n",
        "train_labels_array = np.array(train_labels_list, dtype=np.int64)  # Ensure integer type\n",
        "val_images_array = np.array(val_images_list)\n",
        "val_labels_array = np.array(val_labels_list, dtype=np.int64)  # Ensure integer type\n",
        "\n",
        "# Save the combined dataset as .npz\n",
        "np.savez_compressed(os.path.join(dataset_path, \"combined_dataset.npz\"),\n",
        "                    train_images=train_images_array, train_labels=train_labels_array,\n",
        "                    val_images=val_images_array, val_labels=val_labels_array)\n",
        "\n",
        "print(f\"Combined dataset saved at: {dataset_path}/combined_dataset.npz\")\n",
        "print(f\"Train Samples: {len(train_images_array)}, Validation Samples: {len(val_images_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1sZPpyHjOlv_",
        "outputId": "c7de67cd-e6a5-4b8c-e80d-10203aca6837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/combined_dataset.npz\n",
            "Train Samples: 103811, Validation Samples: 11554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "cxbsN-JePASX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Path to the combined dataset in Google Drive\n",
        "combined_dataset_path = \"/content/drive/MyDrive/MIA_Project/dataset/combined_dataset.npz\"\n",
        "\n",
        "# Load the combined dataset\n",
        "data = np.load(combined_dataset_path)\n",
        "\n",
        "train_images = data[\"train_images\"]\n",
        "train_labels = data[\"train_labels\"]\n",
        "val_images = data[\"val_images\"]\n",
        "val_labels = data[\"val_labels\"]\n",
        "\n",
        "# Check dataset properties\n",
        "print(f\"Train images shape: {train_images.shape}\")\n",
        "print(f\"Validation images shape: {val_images.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels.shape}\")\n",
        "\n",
        "num_classes = train_labels.shape[1] if len(train_labels.shape) > 1 else len(np.unique(train_labels))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Define Custom Dataset\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        # Check if images need resizing to 224x224 for ResNet18\n",
        "        self.images = torch.tensor(images, dtype=torch.float32).unsqueeze(1)  # Add channel dim\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)  # One-hot labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "# Create Dataset Objects\n",
        "train_dataset = MedicalDataset(train_images, train_labels)\n",
        "val_dataset = MedicalDataset(val_images, val_labels)\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "inqeEXTZPFjB",
        "outputId": "bfe920de-4a9f-45bf-d44d-416933257681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (103811, 28, 28)\n",
            "Validation images shape: (11554, 28, 28)\n",
            "Train labels shape: (103811,)\n",
            "Validation labels shape: (11554,)\n",
            "Number of classes: 13\n",
            "Train dataset size: 103811, Validation dataset size: 11554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet18 Cross Validation Training**"
      ],
      "metadata": {
        "id": "HySTRPNI8aXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_dataset.npz\"))\n",
        "\n",
        "train_images, train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "val_images, val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Combine train + val\n",
        "full_images = np.concatenate([train_images, val_images], axis=0)\n",
        "full_labels = np.concatenate([train_labels, val_labels], axis=0)\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create the full dataset\n",
        "full_dataset = CustomDataset(full_images, full_labels)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cross-validation parameters\n",
        "k_folds = 5\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "num_classes = 13\n",
        "\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "best_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Perform cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(full_images, full_labels)):\n",
        "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "\n",
        "    # Create subset datasets\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model, loss, optimizer\n",
        "    model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Fold {fold+1} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# Save best model\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(best_model_state, os.path.join(model_dir, \"resnet18_model_kfold_best.pth\"))\n",
        "print(f\"\\nBest model (acc = {best_acc:.4f}) saved at: resnet18_model_kfold_best.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yojXKrLT8hXr",
        "outputId": "157e4940-f449-4c1a-c539-c0f9f7ac02bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6073, Train Accuracy: 0.7928\n",
            "Epoch [2/10] Loss: 0.4355, Train Accuracy: 0.8484\n",
            "Epoch [3/10] Loss: 0.3839, Train Accuracy: 0.8646\n",
            "Epoch [4/10] Loss: 0.3634, Train Accuracy: 0.8729\n",
            "Epoch [5/10] Loss: 0.3408, Train Accuracy: 0.8812\n",
            "Epoch [6/10] Loss: 0.3284, Train Accuracy: 0.8861\n",
            "Epoch [7/10] Loss: 0.2845, Train Accuracy: 0.9002\n",
            "Epoch [8/10] Loss: 0.2951, Train Accuracy: 0.8987\n",
            "Epoch [9/10] Loss: 0.2554, Train Accuracy: 0.9097\n",
            "Epoch [10/10] Loss: 0.2565, Train Accuracy: 0.9103\n",
            "Fold 1 Validation Accuracy: 0.8729\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.5962, Train Accuracy: 0.7987\n",
            "Epoch [2/10] Loss: 0.4442, Train Accuracy: 0.8474\n",
            "Epoch [3/10] Loss: 0.3971, Train Accuracy: 0.8631\n",
            "Epoch [4/10] Loss: 0.3774, Train Accuracy: 0.8693\n",
            "Epoch [5/10] Loss: 0.3241, Train Accuracy: 0.8851\n",
            "Epoch [6/10] Loss: 0.3213, Train Accuracy: 0.8878\n",
            "Epoch [7/10] Loss: 0.3087, Train Accuracy: 0.8917\n",
            "Epoch [8/10] Loss: 0.2768, Train Accuracy: 0.9030\n",
            "Epoch [9/10] Loss: 0.2565, Train Accuracy: 0.9097\n",
            "Epoch [10/10] Loss: 0.2374, Train Accuracy: 0.9158\n",
            "Fold 2 Validation Accuracy: 0.8991\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6054, Train Accuracy: 0.7970\n",
            "Epoch [2/10] Loss: 0.4580, Train Accuracy: 0.8434\n",
            "Epoch [3/10] Loss: 0.4265, Train Accuracy: 0.8524\n",
            "Epoch [4/10] Loss: 0.3773, Train Accuracy: 0.8684\n",
            "Epoch [5/10] Loss: 0.3510, Train Accuracy: 0.8756\n",
            "Epoch [6/10] Loss: 0.3344, Train Accuracy: 0.8817\n",
            "Epoch [7/10] Loss: 0.3246, Train Accuracy: 0.8857\n",
            "Epoch [8/10] Loss: 0.3030, Train Accuracy: 0.8932\n",
            "Epoch [9/10] Loss: 0.3062, Train Accuracy: 0.8941\n",
            "Epoch [10/10] Loss: 0.3478, Train Accuracy: 0.8789\n",
            "Fold 3 Validation Accuracy: 0.8842\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6042, Train Accuracy: 0.7940\n",
            "Epoch [2/10] Loss: 0.4491, Train Accuracy: 0.8449\n",
            "Epoch [3/10] Loss: 0.3816, Train Accuracy: 0.8670\n",
            "Epoch [4/10] Loss: 0.3868, Train Accuracy: 0.8654\n",
            "Epoch [5/10] Loss: 0.3512, Train Accuracy: 0.8765\n",
            "Epoch [6/10] Loss: 0.3503, Train Accuracy: 0.8767\n",
            "Epoch [7/10] Loss: 0.3518, Train Accuracy: 0.8788\n",
            "Epoch [8/10] Loss: 0.3165, Train Accuracy: 0.8892\n",
            "Epoch [9/10] Loss: 0.3628, Train Accuracy: 0.8731\n",
            "Epoch [10/10] Loss: 0.2805, Train Accuracy: 0.9013\n",
            "Fold 4 Validation Accuracy: 0.8608\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6017, Train Accuracy: 0.7936\n",
            "Epoch [2/10] Loss: 0.4365, Train Accuracy: 0.8484\n",
            "Epoch [3/10] Loss: 0.3965, Train Accuracy: 0.8619\n",
            "Epoch [4/10] Loss: 0.3631, Train Accuracy: 0.8715\n",
            "Epoch [5/10] Loss: 0.3444, Train Accuracy: 0.8793\n",
            "Epoch [6/10] Loss: 0.3407, Train Accuracy: 0.8810\n",
            "Epoch [7/10] Loss: 0.2913, Train Accuracy: 0.8977\n",
            "Epoch [8/10] Loss: 0.2856, Train Accuracy: 0.8993\n",
            "Epoch [9/10] Loss: 0.2752, Train Accuracy: 0.9024\n",
            "Epoch [10/10] Loss: 0.2532, Train Accuracy: 0.9130\n",
            "Fold 5 Validation Accuracy: 0.8864\n",
            "\n",
            "Best model (acc = 0.8991) saved at: resnet18_model_kfold_best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Test Data**"
      ],
      "metadata": {
        "id": "jTbvaqkS0pMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from medmnist import OCTMNIST, PneumoniaMNIST, BreastMNIST, RetinaMNIST\n",
        "from medmnist.info import INFO\n",
        "\n",
        "# Select your data path\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Global label mapping\n",
        "global_label_mapping = {\n",
        "    0: \"choroidal neovascularization\",\n",
        "    1: \"diabetic macular edema\",\n",
        "    2: \"drusen\",\n",
        "    3: \"normal (OCT)\",\n",
        "    4: \"normal (Pneumonia)\",\n",
        "    5: \"pneumonia\",\n",
        "    6: \"malignant\",\n",
        "    7: \"normal, benign (Breast)\",\n",
        "    8: \"Retina 0\",\n",
        "    9: \"Retina 1\",\n",
        "    10: \"Retina 2\",\n",
        "    11: \"Retina 3\",\n",
        "    12: \"Retina 4\"\n",
        "}\n",
        "\n",
        "# Dataset configs\n",
        "DATASETS = {\n",
        "    \"octmnist\": (OCTMNIST, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (PneumoniaMNIST, [4, 5]),\n",
        "    \"breastmnist\": (BreastMNIST, [6, 7]),\n",
        "    \"retinamnist\": (RetinaMNIST, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "# Download and combine test data\n",
        "test_images_list = []\n",
        "test_labels_list = []\n",
        "\n",
        "for name, (dataset_class, label_offsets) in DATASETS.items():\n",
        "    # Download dataset\n",
        "    dataset = dataset_class(split='test', download=True, root=dataset_path)\n",
        "\n",
        "    for img, label in zip(dataset.imgs, dataset.labels):\n",
        "        img_pil = Image.fromarray(img.squeeze())  # Convert to grayscale if needed\n",
        "        img_tensor = transform(img_pil)\n",
        "        class_index = label_offsets[int(label[0])]\n",
        "        test_images_list.append(img_tensor.squeeze(0).numpy())  # remove channel dim\n",
        "        test_labels_list.append(class_index)\n",
        "\n",
        "# Convert lists to arrays\n",
        "test_images_array = np.array(test_images_list)\n",
        "test_labels_array = np.array(test_labels_list, dtype=np.int64)\n",
        "\n",
        "# Save combined test data\n",
        "combined_test_path = os.path.join(dataset_path, \"combined_test_dataset.npz\")\n",
        "np.savez_compressed(combined_test_path,\n",
        "                    test_images=test_images_array,\n",
        "                    test_labels=test_labels_array)\n",
        "\n",
        "print(f\"Combined test dataset saved at: {combined_test_path}\")\n",
        "print(f\"Total Test Samples: {len(test_images_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cFkqisqr0tXJ",
        "outputId": "9e629183-695a-4819-b40a-bb3211876c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined test dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/combined_test_dataset.npz\n",
            "Total Test Samples: 2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Models**"
      ],
      "metadata": {
        "id": "QLCyfpuVu9LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# Load the true test dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_test_dataset.npz\"))\n",
        "\n",
        "test_images, test_labels = data[\"test_images\"], data[\"test_labels\"]\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Test loader\n",
        "batch_size = 64\n",
        "test_dataset = CustomDataset(test_images, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Modified ResNet18 model\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18()\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model_path, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ModifiedResNet18(num_classes=13).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovo\")\n",
        "\n",
        "    print(f\"\\nEvaluation Results: {model_name}\")\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"AUC      : {auc:.4f}\")\n",
        "\n",
        "# Paths\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "base_model_path = os.path.join(model_dir, \"resnet18_model.pth\")\n",
        "finetuned_model_path = os.path.join(model_dir, \"resnet18_model_finetuned.pth\")\n",
        "kfold_model_path = os.path.join(model_dir, \"resnet18_model_kfold_best.pth\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(base_model_path, \"Base Model (Trained Only)\")\n",
        "evaluate_model(finetuned_model_path, \"Fine-tuned Model (Train + Val)\")\n",
        "evaluate_model(kfold_model_path, \"Final Model (K-Fold + Combined Dataset)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvwvVUd8vBUU",
        "outputId": "75ed9002-89e2-421b-888b-7fc7c1641bc4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Base Model (Trained Only)\n",
            "Accuracy : 0.7216\n",
            "Precision: 0.7015\n",
            "Recall   : 0.7216\n",
            "AUC      : 0.9430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Fine-tuned Model (Train + Val)\n",
            "Accuracy : 0.7193\n",
            "Precision: 0.7337\n",
            "Recall   : 0.7193\n",
            "AUC      : 0.9404\n",
            "\n",
            "Evaluation Results: Final Model (K-Fold + Combined Dataset)\n",
            "Accuracy : 0.7394\n",
            "Precision: 0.7281\n",
            "Recall   : 0.7394\n",
            "AUC      : 0.9395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}