{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Google Drive**"
      ],
      "metadata": {
        "id": "kp77jU9pfi7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT-5TCceT-HU",
        "outputId": "2e9ebeca-ad2c-407a-9768-4d5aa373349f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!fusermount -u /content/drive\n",
        "!rm -rf /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Project Directory**"
      ],
      "metadata": {
        "id": "NHBAAXY-ftuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "print(f\"Project directory set at: {project_dir}\")"
      ],
      "metadata": {
        "id": "FrZtR8yHUlKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5e1681-6ca1-4e48-beb8-c9c6b1aed1ef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project directory set at: /content/drive/MyDrive/MIA_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install & Upgrade Required Packages**"
      ],
      "metadata": {
        "id": "V4LHMviSf7qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install --upgrade torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "ktTktR2IVTQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc0a598-f605-43ea-b147-264fd07da254",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medmnist)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medmnist)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medmnist)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medmnist)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medmnist)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medmnist)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=192c08eefdbfcbb0664caf31b08a970220a9b9c26fa75c3b28b4cd745d14c66d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m777.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download, Move and List MedMNIST Datasets**"
      ],
      "metadata": {
        "id": "5j1FQaQBgEA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Temporary storage in Colab\n",
        "temp_dataset_path = \"/content/dataset\"\n",
        "\n",
        "# Google Drive destination\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(temp_dataset_path, exist_ok=True)\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "\n",
        "# List of all 4 datasets\n",
        "DATASETS = [\n",
        "    \"octmnist\",\"pneumoniamnist\", \"breastmnist\", \"retinamnist\"\n",
        "]\n",
        "\n",
        "# Download and save all datasets in temp storage\n",
        "for dataset_name in DATASETS:\n",
        "    dataset_class = getattr(medmnist, INFO[dataset_name][\"python_class\"])\n",
        "\n",
        "    # Download train, val splits\n",
        "    dataset_class(split=\"train\", download=True, root=temp_dataset_path)\n",
        "    dataset_class(split=\"val\", download=True, root=temp_dataset_path)\n",
        "\n",
        "    print(f\"{dataset_name} downloaded in temp storage at {temp_dataset_path}\")\n",
        "\n",
        "# Move datasets from temp storage to Google Drive\n",
        "for file in os.listdir(temp_dataset_path):\n",
        "    shutil.move(os.path.join(temp_dataset_path, file), dataset_path)\n",
        "\n",
        "print(f\"All datasets moved to {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sLgpxPvQrh8B",
        "outputId": "0f4e49af-ccb5-4467-9ad3-6e3a49b971dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54.9M/54.9M [00:04<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "octmnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.17M/4.17M [00:01<00:00, 3.64MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pneumoniamnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 560k/560k [00:00<00:00, 706kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breastmnist downloaded in temp storage at /content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.29M/3.29M [00:01<00:00, 2.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retinamnist downloaded in temp storage at /content/dataset\n",
            "All datasets moved to /content/drive/MyDrive/MIA_Project/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/MIA_Project/dataset\"\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KdFjN3TdWXvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7632c1d-2aa7-438f-8eee-9f5e4ed3f97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 2 root root 4.0K Apr  6 10:36 dataset\n",
            "drwx------ 6 root root 4.0K Apr  6 10:33 drive\n",
            "drwxr-xr-x 1 root root 4.0K Apr  3 13:37 sample_data\n",
            "['pneumoniamnist.npz', 'retinamnist.npz', 'octmnist.npz', 'breastmnist.npz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Information**"
      ],
      "metadata": {
        "id": "4UlTy92eNsLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import numpy as np\n",
        "\n",
        "# Define datasets\n",
        "DATASETS = [\"octmnist\", \"pneumoniamnist\", \"breastmnist\", \"retinamnist\"]\n",
        "\n",
        "# Function to load and inspect dataset\n",
        "def inspect_dataset(dataset_name):\n",
        "    dataset_class = getattr(medmnist, INFO[dataset_name][\"python_class\"])\n",
        "    dataset_train = dataset_class(split=\"train\", root=dataset_path, download=False)\n",
        "    dataset_val = dataset_class(split=\"val\", root=dataset_path, download=False)\n",
        "\n",
        "    print(f\"\\nDataset: {dataset_name}\")\n",
        "    print(f\"Train set size: {len(dataset_train)}\")\n",
        "    print(f\"Validation set size: {len(dataset_val)}\")\n",
        "    # Convert the PIL Image to a NumPy array using np.array\n",
        "    print(f\"Image shape: {np.array(dataset_train[0][0]).shape}\")\n",
        "    print(f\"Number of classes: {len(INFO[dataset_name]['label'])}\")\n",
        "    print(f\"classes: {INFO[dataset_name]['label']}\")\n",
        "\n",
        "# Inspect each dataset\n",
        "for dataset in DATASETS:\n",
        "    inspect_dataset(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HOKmstMqNwUk",
        "outputId": "ed249475-4c6b-4735-ad38-6538bb8c102a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: octmnist\n",
            "Train set size: 97477\n",
            "Validation set size: 10832\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 4\n",
            "classes: {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}\n",
            "\n",
            "Dataset: pneumoniamnist\n",
            "Train set size: 4708\n",
            "Validation set size: 524\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 2\n",
            "classes: {'0': 'normal', '1': 'pneumonia'}\n",
            "\n",
            "Dataset: breastmnist\n",
            "Train set size: 546\n",
            "Validation set size: 78\n",
            "Image shape: (28, 28)\n",
            "Number of classes: 2\n",
            "classes: {'0': 'malignant', '1': 'normal, benign'}\n",
            "\n",
            "Dataset: retinamnist\n",
            "Train set size: 1080\n",
            "Validation set size: 120\n",
            "Image shape: (28, 28, 3)\n",
            "Number of classes: 5\n",
            "classes: {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Techniques**"
      ],
      "metadata": {
        "id": "15R2jjJ3WrIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset path\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Dataset list\n",
        "DATASETS = {\n",
        "    \"octmnist\": (4, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (2, [4, 5]),\n",
        "    \"breastmnist\": (2, [6, 7]),\n",
        "    \"retinamnist\": (5, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"\n",
        "    Applies Gaussian blur with kernel size (3, 3) to the image.\n",
        "    \"\"\"\n",
        "    img = img.astype(np.uint8)\n",
        "    blurred = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "    return blurred\n",
        "\n",
        "# Plotting: Original and enhanced image per dataset\n",
        "fig, axes = plt.subplots(len(DATASETS), 2, figsize=(8, 10))\n",
        "fig.suptitle(\"Original vs Gaussian Blurred (3x3)\", fontsize=16)\n",
        "\n",
        "row = 0\n",
        "for dataset_name in DATASETS.keys():\n",
        "    dataset_file = os.path.join(dataset_path, f\"{dataset_name}.npz\")\n",
        "    with np.load(dataset_file, allow_pickle=True) as data:\n",
        "        train_images = data[\"train_images\"]\n",
        "\n",
        "    # Convert RGB to grayscale if needed\n",
        "    if train_images.ndim == 4 and train_images.shape[-1] == 3:\n",
        "        train_images = np.mean(train_images, axis=-1)\n",
        "\n",
        "    # Use the first image for each dataset\n",
        "    original = train_images[0]\n",
        "    enhanced = preprocess_image(original)\n",
        "\n",
        "    for col, img in enumerate([original, enhanced]):\n",
        "        ax = axes[row, col]\n",
        "        ax.imshow(img, cmap=\"gray\")\n",
        "        ax.axis(\"off\")\n",
        "        if row == 0:\n",
        "            ax.set_title([\"Original\", \"Gaussian Blurred\"][col])\n",
        "\n",
        "    axes[row, 0].set_ylabel(dataset_name.upper(), fontsize=13, rotation=90, labelpad=10)\n",
        "    row += 1\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UHy-XxeYWwmB",
        "outputId": "313fdfba-2572-48ca-ecb9-7084a39629d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combine Datasets**"
      ],
      "metadata": {
        "id": "hOJR0vzyOiCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Path where datasets are saved\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Global label mapping\n",
        "global_label_mapping = {\n",
        "    0: \"choroidal neovascularization\",\n",
        "    1: \"diabetic macular edema\",\n",
        "    2: \"drusen\",\n",
        "    3: \"normal (OCT)\",\n",
        "    4: \"normal (Pneumonia)\",\n",
        "    5: \"pneumonia\",\n",
        "    6: \"malignant\",\n",
        "    7: \"normal, benign (Breast)\",\n",
        "    8: \"Retina 0\",\n",
        "    9: \"Retina 1\",\n",
        "    10: \"Retina 2\",\n",
        "    11: \"Retina 3\",\n",
        "    12: \"Retina 4\"\n",
        "}\n",
        "\n",
        "# List of datasets with label offsets\n",
        "DATASETS = {\n",
        "    \"octmnist\": (4, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (2, [4, 5]),\n",
        "    \"breastmnist\": (2, [6, 7]),\n",
        "    \"retinamnist\": (5, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "# Storage for combined images and labels\n",
        "train_images_list, train_labels_list = [], []\n",
        "val_images_list, val_labels_list = [], []\n",
        "\n",
        "# Load and process each dataset\n",
        "for dataset_name, (num_classes, label_offsets) in DATASETS.items():\n",
        "    dataset_file = os.path.join(dataset_path, f\"{dataset_name}.npz\")\n",
        "\n",
        "    # Load .npz file\n",
        "    with np.load(dataset_file, allow_pickle=True) as data:\n",
        "        train_images = data[\"train_images\"]\n",
        "        train_labels = data[\"train_labels\"]\n",
        "        val_images = data[\"val_images\"]\n",
        "        val_labels = data[\"val_labels\"]\n",
        "\n",
        "    # Convert to grayscale if necessary\n",
        "    if train_images.ndim == 4 and train_images.shape[-1] == 3:  # RGB -> Grayscale\n",
        "        train_images = np.mean(train_images, axis=-1)\n",
        "        val_images = np.mean(val_images, axis=-1)\n",
        "\n",
        "    # Normalize & Store Label Index\n",
        "    for img, label in zip(train_images, train_labels):\n",
        "        img_pil = Image.fromarray(img.astype(np.uint8))  # Convert NumPy array to PIL Image\n",
        "        img_tensor = transform(img_pil)  # Apply transform to PIL Image\n",
        "        class_index = label_offsets[int(label[0])]  # Get class index\n",
        "        train_images_list.append(img_tensor.squeeze(0).numpy())  # Remove singleton dim\n",
        "        train_labels_list.append(class_index)\n",
        "\n",
        "    for img, label in zip(val_images, val_labels):\n",
        "        img_pil = Image.fromarray(img.astype(np.uint8))  # Convert NumPy array to PIL Image\n",
        "        img_tensor = transform(img_pil)  # Apply transform to PIL Image\n",
        "        class_index = label_offsets[int(label[0])]  # Get class index\n",
        "        val_images_list.append(img_tensor.squeeze(0).numpy())\n",
        "        val_labels_list.append(class_index)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "train_images_array = np.array(train_images_list)\n",
        "train_labels_array = np.array(train_labels_list, dtype=np.int64)  # Ensure integer type\n",
        "val_images_array = np.array(val_images_list)\n",
        "val_labels_array = np.array(val_labels_list, dtype=np.int64)  # Ensure integer type\n",
        "\n",
        "# Save the combined dataset as .npz\n",
        "np.savez_compressed(os.path.join(dataset_path, \"combined_dataset.npz\"),\n",
        "                    train_images=train_images_array, train_labels=train_labels_array,\n",
        "                    val_images=val_images_array, val_labels=val_labels_array)\n",
        "\n",
        "print(f\"Combined dataset saved at: {dataset_path}/combined_dataset.npz\")\n",
        "print(f\"Train Samples: {len(train_images_array)}, Validation Samples: {len(val_images_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1sZPpyHjOlv_",
        "outputId": "c7de67cd-e6a5-4b8c-e80d-10203aca6837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/combined_dataset.npz\n",
            "Train Samples: 103811, Validation Samples: 11554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combine (Preprocessed) Datasets**"
      ],
      "metadata": {
        "id": "WxXto9TXOSWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Path where datasets are saved\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# List of datasets with label offsets\n",
        "DATASETS = {\n",
        "    \"octmnist\": (4, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (2, [4, 5]),\n",
        "    \"breastmnist\": (2, [6, 7]),\n",
        "    \"retinamnist\": (5, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "# Storage for images and labels\n",
        "original_train_images_list, train_images_list, train_labels_list = [], [], []\n",
        "original_val_images_list, val_images_list, val_labels_list = [], [], []\n",
        "\n",
        "def apply_gaussian_blur(img):\n",
        "    \"\"\"\n",
        "    Apply Gaussian blur with kernel size (3, 3).\n",
        "    \"\"\"\n",
        "    img = img.astype(np.uint8)\n",
        "    return cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "# Load and process each dataset\n",
        "for dataset_name, (num_classes, label_offsets) in DATASETS.items():\n",
        "    dataset_file = os.path.join(dataset_path, f\"{dataset_name}.npz\")\n",
        "\n",
        "    with np.load(dataset_file, allow_pickle=True) as data:\n",
        "        train_images = data[\"train_images\"]\n",
        "        train_labels = data[\"train_labels\"]\n",
        "        val_images = data[\"val_images\"]\n",
        "        val_labels = data[\"val_labels\"]\n",
        "\n",
        "    if train_images.ndim == 4 and train_images.shape[-1] == 3:\n",
        "        train_images = np.mean(train_images, axis=-1)\n",
        "        val_images = np.mean(val_images, axis=-1)\n",
        "\n",
        "    for img, label in zip(train_images, train_labels):\n",
        "        img = img.astype(np.uint8)\n",
        "        original_train_images_list.append(img.copy())\n",
        "\n",
        "        enhanced = apply_gaussian_blur(img)\n",
        "        img_tensor = transform(Image.fromarray(enhanced))\n",
        "\n",
        "        class_index = label_offsets[int(label[0])]\n",
        "        train_images_list.append(img_tensor.squeeze(0).numpy())\n",
        "        train_labels_list.append(class_index)\n",
        "\n",
        "    for img, label in zip(val_images, val_labels):\n",
        "        img = img.astype(np.uint8)\n",
        "        original_val_images_list.append(img.copy())\n",
        "\n",
        "        enhanced = apply_gaussian_blur(img)\n",
        "        img_tensor = transform(Image.fromarray(enhanced))\n",
        "\n",
        "        class_index = label_offsets[int(label[0])]\n",
        "        val_images_list.append(img_tensor.squeeze(0).numpy())\n",
        "        val_labels_list.append(class_index)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "original_train_images_array = np.array(original_train_images_list)\n",
        "train_images_array = np.array(train_images_list)\n",
        "train_labels_array = np.array(train_labels_list, dtype=np.int64)\n",
        "\n",
        "original_val_images_array = np.array(original_val_images_list)\n",
        "val_images_array = np.array(val_images_list)\n",
        "val_labels_array = np.array(val_labels_list, dtype=np.int64)\n",
        "\n",
        "# Save the enhanced dataset\n",
        "np.savez_compressed(os.path.join(dataset_path, \"pre_combined_dataset.npz\"),\n",
        "                    train_images=train_images_array, train_labels=train_labels_array,\n",
        "                    val_images=val_images_array, val_labels=val_labels_array)\n",
        "\n",
        "print(f\"Enhanced dataset saved at: {dataset_path}/pre_combined_dataset.npz\")\n",
        "print(f\"Train Samples: {len(train_images_array)}, Validation Samples: {len(val_images_array)}\")\n",
        "\n",
        "# Display 5 images before and after enhancement\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "for i in range(5):\n",
        "    axes[0, i].imshow(original_train_images_array[i], cmap=\"gray\")\n",
        "    axes[0, i].set_title(\"Before\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    axes[1, i].imshow(train_images_array[i], cmap=\"gray\")\n",
        "    axes[1, i].set_title(\"After\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "collapsed": true,
        "id": "6bdAsS9qOa8x",
        "outputId": "ca80c931-37ab-4294-c90e-7432048c62aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/pre_combined_dataset.npz\n",
            "Train Samples: 103811, Validation Samples: 11554\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "cxbsN-JePASX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Path to the combined dataset in Google Drive\n",
        "combined_dataset_path = \"/content/drive/MyDrive/MIA_Project/dataset/combined_dataset.npz\"\n",
        "\n",
        "# Load the combined dataset\n",
        "data = np.load(combined_dataset_path)\n",
        "\n",
        "train_images = data[\"train_images\"]\n",
        "train_labels = data[\"train_labels\"]\n",
        "val_images = data[\"val_images\"]\n",
        "val_labels = data[\"val_labels\"]\n",
        "\n",
        "# Check dataset properties\n",
        "print(f\"Train images shape: {train_images.shape}\")\n",
        "print(f\"Validation images shape: {val_images.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels.shape}\")\n",
        "\n",
        "num_classes = train_labels.shape[1] if len(train_labels.shape) > 1 else len(np.unique(train_labels))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Define Custom Dataset\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        # Check if images need resizing to 224x224 for ResNet18\n",
        "        self.images = torch.tensor(images, dtype=torch.float32).unsqueeze(1)  # Add channel dim\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)  # One-hot labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "# Create Dataset Objects\n",
        "train_dataset = MedicalDataset(train_images, train_labels)\n",
        "val_dataset = MedicalDataset(val_images, val_labels)\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "inqeEXTZPFjB",
        "outputId": "bfe920de-4a9f-45bf-d44d-416933257681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (103811, 28, 28)\n",
            "Validation images shape: (11554, 28, 28)\n",
            "Train labels shape: (103811,)\n",
            "Validation labels shape: (11554,)\n",
            "Number of classes: 13\n",
            "Train dataset size: 103811, Validation dataset size: 11554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load (Preprocessed) Dataset**"
      ],
      "metadata": {
        "id": "4m1LXtZ4itcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "#  Path to the enhanced dataset\n",
        "enhanced_dataset_path = \"/content/drive/MyDrive/MIA_Project/dataset/pre_combined_dataset.npz\"\n",
        "\n",
        "#  Load the enhanced dataset\n",
        "data = np.load(enhanced_dataset_path)\n",
        "\n",
        "pre_train_images = data[\"train_images\"]\n",
        "pre_train_labels = data[\"train_labels\"]\n",
        "pre_val_images = data[\"val_images\"]\n",
        "pre_val_labels = data[\"val_labels\"]\n",
        "\n",
        "#  Check dataset properties\n",
        "print(f\"Train images shape: {pre_train_images.shape}\")\n",
        "print(f\"Validation images shape: {pre_val_images.shape}\")\n",
        "print(f\"Train labels shape: {pre_train_labels.shape}\")\n",
        "print(f\"Validation labels shape: {pre_val_labels.shape}\")\n",
        "\n",
        "pre_num_classes = pre_train_labels.shape[1] if len(pre_train_labels.shape) > 1 else len(np.unique(pre_train_labels))\n",
        "print(f\"Number of classes: {pre_num_classes}\")\n",
        "\n",
        "#  Custom Dataset\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = torch.tensor(images, dtype=torch.float32).unsqueeze(1)  # Add channel dim (1xHxW)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)  # Integer class labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "#  Create Dataset objects\n",
        "pre_train_dataset = MedicalDataset(pre_train_images, pre_train_labels)\n",
        "pre_val_dataset = MedicalDataset(pre_val_images, pre_val_labels)\n",
        "\n",
        "#  DataLoaders\n",
        "pre_train_loader = DataLoader(pre_train_dataset, batch_size=32, shuffle=True)\n",
        "pre_val_loader = DataLoader(pre_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset size: {len(pre_train_dataset)}, Validation dataset size: {len(pre_val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DsCp_m6ei1Kq",
        "outputId": "bfc6a856-2852-4b6e-f0ff-fd838f3ec53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (103811, 28, 28)\n",
            "Validation images shape: (11554, 28, 28)\n",
            "Train labels shape: (103811,)\n",
            "Validation labels shape: (11554,)\n",
            "Number of classes: 13\n",
            "Train dataset size: 103811, Validation dataset size: 11554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train ResNet18**"
      ],
      "metadata": {
        "id": "lYYMPyA6Hu5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_dataset.npz\"))\n",
        "\n",
        "train_images, train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "val_images, val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        # Convert grayscale (H, W) → (1, H, W)\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "batch_size = 64\n",
        "train_dataset = CustomDataset(train_images, train_labels)\n",
        "val_dataset = CustomDataset(val_images, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Modify first convolution layer to accept grayscale input (1 channel)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Modify final layer for classification\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = 13\n",
        "model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_dir = \"/content/drive/MyDrive/MIA_Project/model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"resnet18_model.pth\"))\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dNIzsueFHzfM",
        "outputId": "05f9f706-8730-4591-d050-72422baacb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5891, Train Acc: 0.8014\n",
            "Epoch [2/10], Loss: 0.4264, Train Acc: 0.8526\n",
            "Epoch [3/10], Loss: 0.3684, Train Acc: 0.8715\n",
            "Epoch [4/10], Loss: 0.3691, Train Acc: 0.8748\n",
            "Epoch [5/10], Loss: 0.3526, Train Acc: 0.8787\n",
            "Epoch [6/10], Loss: 0.3215, Train Acc: 0.8883\n",
            "Epoch [7/10], Loss: 0.3049, Train Acc: 0.8932\n",
            "Epoch [8/10], Loss: 0.2718, Train Acc: 0.9051\n",
            "Epoch [9/10], Loss: 0.2779, Train Acc: 0.9026\n",
            "Epoch [10/10], Loss: 0.2391, Train Acc: 0.9162\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train ResNet18 (Preprocessed)**"
      ],
      "metadata": {
        "id": "yr-D5uVXjd80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"pre_combined_dataset.npz\"))\n",
        "\n",
        "pre_train_images, pre_train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "pre_val_images, pre_val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        # Convert grayscale (H, W) → (1, H, W)\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "batch_size = 64\n",
        "pre_train_dataset = CustomDataset(pre_train_images, pre_train_labels)\n",
        "pre_val_dataset = CustomDataset(pre_val_images, pre_val_labels)\n",
        "\n",
        "pre_train_loader = DataLoader(pre_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "pre_val_loader = DataLoader(pre_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Modify first convolution layer to accept grayscale input (1 channel)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Modify final layer for classification\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = 13\n",
        "model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in pre_train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    pre_train_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(pre_train_loader):.4f}, Train Acc: {pre_train_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_dir = \"/content/drive/MyDrive/MIA_Project/model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"resnet18_preprocessed_model.pth\"))\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ae4356e2-49e2-4195-9bef-c5cc15c5ad51",
        "id": "QzL8KD2OkUNG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 183MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6317, Train Acc: 0.7843\n",
            "Epoch [2/10], Loss: 0.4726, Train Acc: 0.8374\n",
            "Epoch [3/10], Loss: 0.4111, Train Acc: 0.8558\n",
            "Epoch [4/10], Loss: 0.3823, Train Acc: 0.8662\n",
            "Epoch [5/10], Loss: 0.3836, Train Acc: 0.8649\n",
            "Epoch [6/10], Loss: 0.3456, Train Acc: 0.8770\n",
            "Epoch [7/10], Loss: 0.3430, Train Acc: 0.8803\n",
            "Epoch [8/10], Loss: 0.3035, Train Acc: 0.8922\n",
            "Epoch [9/10], Loss: 0.3060, Train Acc: 0.8917\n",
            "Epoch [10/10], Loss: 0.2608, Train Acc: 0.9081\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finetune ResNet18 on Validation Data**"
      ],
      "metadata": {
        "id": "kVnDI32Shzj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_dataset.npz\"))\n",
        "\n",
        "train_images, train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "val_images, val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "batch_size = 64\n",
        "train_dataset = CustomDataset(train_images, train_labels)\n",
        "val_dataset = CustomDataset(val_images, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = 13\n",
        "model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 1: Train on training data\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "# Step 2: Fine-tune on validation data\n",
        "print(\"\\nStarting fine-tuning on validation data...\\n\")\n",
        "finetune_epochs = 3\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Smaller LR for fine-tuning\n",
        "\n",
        "for epoch in range(finetune_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    print(f\"Fine-tune Epoch [{epoch+1}/{finetune_epochs}] Loss: {running_loss/len(val_loader):.4f}, Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_dir = \"/content/drive/MyDrive/MIA_Project/model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"resnet18_model_finetuned.pth\"))\n",
        "print(\"Model (fine-tuned) saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y1TJHl3oh2yu",
        "outputId": "bc2a4d07-1d27-4511-f15e-2e301c4d4f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Training Loss: 0.5887, Accuracy: 0.8006\n",
            "Epoch [2/10] Training Loss: 0.4273, Accuracy: 0.8517\n",
            "Epoch [3/10] Training Loss: 0.3663, Accuracy: 0.8709\n",
            "Epoch [4/10] Training Loss: 0.3364, Accuracy: 0.8814\n",
            "Epoch [5/10] Training Loss: 0.3210, Accuracy: 0.8876\n",
            "Epoch [6/10] Training Loss: 0.2914, Accuracy: 0.8972\n",
            "Epoch [7/10] Training Loss: 0.3202, Accuracy: 0.8892\n",
            "Epoch [8/10] Training Loss: 0.3166, Accuracy: 0.8883\n",
            "Epoch [9/10] Training Loss: 0.2605, Accuracy: 0.9083\n",
            "Epoch [10/10] Training Loss: 0.2397, Accuracy: 0.9154\n",
            "\n",
            "Starting fine-tuning on validation data...\n",
            "\n",
            "Fine-tune Epoch [1/3] Loss: 0.2523, Accuracy: 0.9122\n",
            "Fine-tune Epoch [2/3] Loss: 0.2287, Accuracy: 0.9214\n",
            "Fine-tune Epoch [3/3] Loss: 0.2100, Accuracy: 0.9282\n",
            "Model (fine-tuned) saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finetune ResNet18 on Validation Data (Preprocessed)**"
      ],
      "metadata": {
        "id": "hqaviJEAoMlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"pre_combined_dataset.npz\"))\n",
        "\n",
        "pre_train_images, pre_train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "pre_val_images, pre_val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "batch_size = 64\n",
        "pre_train_dataset = CustomDataset(pre_train_images, pre_train_labels)\n",
        "pre_val_dataset = CustomDataset(pre_val_images, pre_val_labels)\n",
        "\n",
        "pre_train_loader = DataLoader(pre_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "pre_val_loader = DataLoader(pre_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "num_classes = 13\n",
        "model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 1: Train on training data\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in pre_train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    pre_train_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {running_loss/len(pre_train_loader):.4f}, Accuracy: {pre_train_acc:.4f}\")\n",
        "\n",
        "# Step 2: Fine-tune on validation data\n",
        "print(\"\\nStarting fine-tuning on validation data...\\n\")\n",
        "finetune_epochs = 3\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Smaller LR for fine-tuning\n",
        "\n",
        "for epoch in range(finetune_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in pre_val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    pre_val_acc = correct / total\n",
        "    print(f\"Fine-tune Epoch [{epoch+1}/{finetune_epochs}] Loss: {running_loss/len(pre_val_loader):.4f}, Accuracy: {pre_val_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_dir = \"/content/drive/MyDrive/MIA_Project/model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"resnet18_preprocessed_model_finetuned.pth\"))\n",
        "print(\"Model (fine-tuned) saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1dd4d03f-b1e5-4ca9-b2ac-4e556ae478ea",
        "id": "YSoF_-EBoUiK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Training Loss: 0.6522, Accuracy: 0.7798\n",
            "Epoch [2/10] Training Loss: 0.5126, Accuracy: 0.8242\n",
            "Epoch [3/10] Training Loss: 0.4930, Accuracy: 0.8304\n",
            "Epoch [4/10] Training Loss: 0.4746, Accuracy: 0.8346\n",
            "Epoch [5/10] Training Loss: 0.4031, Accuracy: 0.8590\n",
            "Epoch [6/10] Training Loss: 0.4403, Accuracy: 0.8474\n",
            "Epoch [7/10] Training Loss: 0.3774, Accuracy: 0.8680\n",
            "Epoch [8/10] Training Loss: 0.3714, Accuracy: 0.8690\n",
            "Epoch [9/10] Training Loss: 0.3216, Accuracy: 0.8861\n",
            "Epoch [10/10] Training Loss: 0.3638, Accuracy: 0.8754\n",
            "\n",
            "Starting fine-tuning on validation data...\n",
            "\n",
            "Fine-tune Epoch [1/3] Loss: 0.4240, Accuracy: 0.8513\n",
            "Fine-tune Epoch [2/3] Loss: 0.3458, Accuracy: 0.8764\n",
            "Fine-tune Epoch [3/3] Loss: 0.3227, Accuracy: 0.8859\n",
            "Model (fine-tuned) saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet18 Cross Validation Training**"
      ],
      "metadata": {
        "id": "HySTRPNI8aXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_dataset.npz\"))\n",
        "\n",
        "train_images, train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "val_images, val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Combine train + val\n",
        "full_images = np.concatenate([train_images, val_images], axis=0)\n",
        "full_labels = np.concatenate([train_labels, val_labels], axis=0)\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create the full dataset\n",
        "full_dataset = CustomDataset(full_images, full_labels)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cross-validation parameters\n",
        "k_folds = 5\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "num_classes = 13\n",
        "\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "best_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Perform cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(full_images, full_labels)):\n",
        "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "\n",
        "    # Create subset datasets\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model, loss, optimizer\n",
        "    model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Fold {fold+1} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# Save best model\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(best_model_state, os.path.join(model_dir, \"resnet18_model_kfold_best.pth\"))\n",
        "print(f\"\\nBest model (acc = {best_acc:.4f}) saved at: resnet18_model_kfold_best.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yojXKrLT8hXr",
        "outputId": "157e4940-f449-4c1a-c539-c0f9f7ac02bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6073, Train Accuracy: 0.7928\n",
            "Epoch [2/10] Loss: 0.4355, Train Accuracy: 0.8484\n",
            "Epoch [3/10] Loss: 0.3839, Train Accuracy: 0.8646\n",
            "Epoch [4/10] Loss: 0.3634, Train Accuracy: 0.8729\n",
            "Epoch [5/10] Loss: 0.3408, Train Accuracy: 0.8812\n",
            "Epoch [6/10] Loss: 0.3284, Train Accuracy: 0.8861\n",
            "Epoch [7/10] Loss: 0.2845, Train Accuracy: 0.9002\n",
            "Epoch [8/10] Loss: 0.2951, Train Accuracy: 0.8987\n",
            "Epoch [9/10] Loss: 0.2554, Train Accuracy: 0.9097\n",
            "Epoch [10/10] Loss: 0.2565, Train Accuracy: 0.9103\n",
            "Fold 1 Validation Accuracy: 0.8729\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.5962, Train Accuracy: 0.7987\n",
            "Epoch [2/10] Loss: 0.4442, Train Accuracy: 0.8474\n",
            "Epoch [3/10] Loss: 0.3971, Train Accuracy: 0.8631\n",
            "Epoch [4/10] Loss: 0.3774, Train Accuracy: 0.8693\n",
            "Epoch [5/10] Loss: 0.3241, Train Accuracy: 0.8851\n",
            "Epoch [6/10] Loss: 0.3213, Train Accuracy: 0.8878\n",
            "Epoch [7/10] Loss: 0.3087, Train Accuracy: 0.8917\n",
            "Epoch [8/10] Loss: 0.2768, Train Accuracy: 0.9030\n",
            "Epoch [9/10] Loss: 0.2565, Train Accuracy: 0.9097\n",
            "Epoch [10/10] Loss: 0.2374, Train Accuracy: 0.9158\n",
            "Fold 2 Validation Accuracy: 0.8991\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6054, Train Accuracy: 0.7970\n",
            "Epoch [2/10] Loss: 0.4580, Train Accuracy: 0.8434\n",
            "Epoch [3/10] Loss: 0.4265, Train Accuracy: 0.8524\n",
            "Epoch [4/10] Loss: 0.3773, Train Accuracy: 0.8684\n",
            "Epoch [5/10] Loss: 0.3510, Train Accuracy: 0.8756\n",
            "Epoch [6/10] Loss: 0.3344, Train Accuracy: 0.8817\n",
            "Epoch [7/10] Loss: 0.3246, Train Accuracy: 0.8857\n",
            "Epoch [8/10] Loss: 0.3030, Train Accuracy: 0.8932\n",
            "Epoch [9/10] Loss: 0.3062, Train Accuracy: 0.8941\n",
            "Epoch [10/10] Loss: 0.3478, Train Accuracy: 0.8789\n",
            "Fold 3 Validation Accuracy: 0.8842\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6042, Train Accuracy: 0.7940\n",
            "Epoch [2/10] Loss: 0.4491, Train Accuracy: 0.8449\n",
            "Epoch [3/10] Loss: 0.3816, Train Accuracy: 0.8670\n",
            "Epoch [4/10] Loss: 0.3868, Train Accuracy: 0.8654\n",
            "Epoch [5/10] Loss: 0.3512, Train Accuracy: 0.8765\n",
            "Epoch [6/10] Loss: 0.3503, Train Accuracy: 0.8767\n",
            "Epoch [7/10] Loss: 0.3518, Train Accuracy: 0.8788\n",
            "Epoch [8/10] Loss: 0.3165, Train Accuracy: 0.8892\n",
            "Epoch [9/10] Loss: 0.3628, Train Accuracy: 0.8731\n",
            "Epoch [10/10] Loss: 0.2805, Train Accuracy: 0.9013\n",
            "Fold 4 Validation Accuracy: 0.8608\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6017, Train Accuracy: 0.7936\n",
            "Epoch [2/10] Loss: 0.4365, Train Accuracy: 0.8484\n",
            "Epoch [3/10] Loss: 0.3965, Train Accuracy: 0.8619\n",
            "Epoch [4/10] Loss: 0.3631, Train Accuracy: 0.8715\n",
            "Epoch [5/10] Loss: 0.3444, Train Accuracy: 0.8793\n",
            "Epoch [6/10] Loss: 0.3407, Train Accuracy: 0.8810\n",
            "Epoch [7/10] Loss: 0.2913, Train Accuracy: 0.8977\n",
            "Epoch [8/10] Loss: 0.2856, Train Accuracy: 0.8993\n",
            "Epoch [9/10] Loss: 0.2752, Train Accuracy: 0.9024\n",
            "Epoch [10/10] Loss: 0.2532, Train Accuracy: 0.9130\n",
            "Fold 5 Validation Accuracy: 0.8864\n",
            "\n",
            "Best model (acc = 0.8991) saved at: resnet18_model_kfold_best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet18 Cross Validation Training (Preprocessed)**"
      ],
      "metadata": {
        "id": "gRFDp7904mD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"pre_combined_dataset.npz\"))\n",
        "\n",
        "pre_train_images, pre_train_labels = data[\"train_images\"], data[\"train_labels\"]\n",
        "pre_val_images, pre_val_labels = data[\"val_images\"], data[\"val_labels\"]\n",
        "\n",
        "# Combine train + val\n",
        "pre_full_images = np.concatenate([pre_train_images, pre_val_images], axis=0)\n",
        "pre_full_labels = np.concatenate([pre_train_labels, pre_val_labels], axis=0)\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Create the full dataset\n",
        "pre_full_dataset = CustomDataset(pre_full_images, pre_full_labels)\n",
        "\n",
        "# Modify ResNet18 to accept grayscale images\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cross-validation parameters\n",
        "k_folds = 5\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "num_classes = 13\n",
        "\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "best_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Perform cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(pre_full_images, pre_full_labels)):\n",
        "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "\n",
        "    # Create subset datasets\n",
        "    pre_train_subset = Subset(pre_full_dataset, train_idx)\n",
        "    pre_val_subset = Subset(pre_full_dataset, val_idx)\n",
        "\n",
        "    pre_train_loader = DataLoader(pre_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    pre_val_loader = DataLoader(pre_val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model, loss, optimizer\n",
        "    model = ModifiedResNet18(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in pre_train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        pre_train_acc = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(pre_train_loader):.4f}, Train Accuracy: {pre_train_acc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pre_val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    pre_val_acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Fold {fold+1} Validation Accuracy: {pre_val_acc:.4f}\")\n",
        "\n",
        "    if pre_val_acc > best_acc:\n",
        "        best_acc = pre_val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "# Save best model\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(best_model_state, os.path.join(model_dir, \"resnet18_preprocessed_model_kfold_best.pth\"))\n",
        "print(f\"\\nBest model (acc = {best_acc:.4f}) saved at: resnet18_preprocessed_model_kfold_best.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314a707f-0ab2-40cb-929d-7d57ccd02d3f",
        "id": "XQ7ff9sa40In",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6622, Train Accuracy: 0.7775\n",
            "Epoch [2/10] Loss: 0.5378, Train Accuracy: 0.8179\n",
            "Epoch [3/10] Loss: 0.4730, Train Accuracy: 0.8359\n",
            "Epoch [4/10] Loss: 0.4651, Train Accuracy: 0.8392\n",
            "Epoch [5/10] Loss: 0.4209, Train Accuracy: 0.8524\n",
            "Epoch [6/10] Loss: 0.4199, Train Accuracy: 0.8541\n",
            "Epoch [7/10] Loss: 0.3758, Train Accuracy: 0.8667\n",
            "Epoch [8/10] Loss: 0.4332, Train Accuracy: 0.8500\n",
            "Epoch [9/10] Loss: 0.3927, Train Accuracy: 0.8644\n",
            "Epoch [10/10] Loss: 0.3830, Train Accuracy: 0.8665\n",
            "Fold 1 Validation Accuracy: 0.8386\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6314, Train Accuracy: 0.7848\n",
            "Epoch [2/10] Loss: 0.4761, Train Accuracy: 0.8337\n",
            "Epoch [3/10] Loss: 0.4326, Train Accuracy: 0.8479\n",
            "Epoch [4/10] Loss: 0.3963, Train Accuracy: 0.8602\n",
            "Epoch [5/10] Loss: 0.3834, Train Accuracy: 0.8633\n",
            "Epoch [6/10] Loss: 0.3752, Train Accuracy: 0.8685\n",
            "Epoch [7/10] Loss: 0.3270, Train Accuracy: 0.8840\n",
            "Epoch [8/10] Loss: 0.3071, Train Accuracy: 0.8915\n",
            "Epoch [9/10] Loss: 0.2849, Train Accuracy: 0.8993\n",
            "Epoch [10/10] Loss: 0.3216, Train Accuracy: 0.8879\n",
            "Fold 2 Validation Accuracy: 0.8887\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6365, Train Accuracy: 0.7779\n",
            "Epoch [2/10] Loss: 0.4745, Train Accuracy: 0.8363\n",
            "Epoch [3/10] Loss: 0.4270, Train Accuracy: 0.8511\n",
            "Epoch [4/10] Loss: 0.3867, Train Accuracy: 0.8633\n",
            "Epoch [5/10] Loss: 0.3711, Train Accuracy: 0.8685\n",
            "Epoch [6/10] Loss: 0.3486, Train Accuracy: 0.8751\n",
            "Epoch [7/10] Loss: 0.3272, Train Accuracy: 0.8838\n",
            "Epoch [8/10] Loss: 0.2993, Train Accuracy: 0.8922\n",
            "Epoch [9/10] Loss: 0.2829, Train Accuracy: 0.9005\n",
            "Epoch [10/10] Loss: 0.2847, Train Accuracy: 0.9012\n",
            "Fold 3 Validation Accuracy: 0.8523\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6755, Train Accuracy: 0.7702\n",
            "Epoch [2/10] Loss: 0.5429, Train Accuracy: 0.8149\n",
            "Epoch [3/10] Loss: 0.4631, Train Accuracy: 0.8388\n",
            "Epoch [4/10] Loss: 0.4844, Train Accuracy: 0.8317\n",
            "Epoch [5/10] Loss: 0.4503, Train Accuracy: 0.8416\n",
            "Epoch [6/10] Loss: 0.4284, Train Accuracy: 0.8501\n",
            "Epoch [7/10] Loss: 0.3903, Train Accuracy: 0.8613\n",
            "Epoch [8/10] Loss: 0.4034, Train Accuracy: 0.8593\n",
            "Epoch [9/10] Loss: 0.3568, Train Accuracy: 0.8728\n",
            "Epoch [10/10] Loss: 0.3536, Train Accuracy: 0.8745\n",
            "Fold 4 Validation Accuracy: 0.8782\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 0.6304, Train Accuracy: 0.7839\n",
            "Epoch [2/10] Loss: 0.4999, Train Accuracy: 0.8292\n",
            "Epoch [3/10] Loss: 0.4818, Train Accuracy: 0.8309\n",
            "Epoch [4/10] Loss: 0.4034, Train Accuracy: 0.8581\n",
            "Epoch [5/10] Loss: 0.4017, Train Accuracy: 0.8577\n",
            "Epoch [6/10] Loss: 0.3533, Train Accuracy: 0.8746\n",
            "Epoch [7/10] Loss: 0.3284, Train Accuracy: 0.8830\n",
            "Epoch [8/10] Loss: 0.3286, Train Accuracy: 0.8848\n",
            "Epoch [9/10] Loss: 0.2932, Train Accuracy: 0.8967\n",
            "Epoch [10/10] Loss: 0.2897, Train Accuracy: 0.8983\n",
            "Fold 5 Validation Accuracy: 0.8643\n",
            "\n",
            "Best model (acc = 0.8887) saved at: resnet18_preprocessed_model_kfold_best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Test Data**"
      ],
      "metadata": {
        "id": "jTbvaqkS0pMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from medmnist import OCTMNIST, PneumoniaMNIST, BreastMNIST, RetinaMNIST\n",
        "from medmnist.info import INFO\n",
        "\n",
        "# Select your data path\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Global label mapping\n",
        "global_label_mapping = {\n",
        "    0: \"choroidal neovascularization\",\n",
        "    1: \"diabetic macular edema\",\n",
        "    2: \"drusen\",\n",
        "    3: \"normal (OCT)\",\n",
        "    4: \"normal (Pneumonia)\",\n",
        "    5: \"pneumonia\",\n",
        "    6: \"malignant\",\n",
        "    7: \"normal, benign (Breast)\",\n",
        "    8: \"Retina 0\",\n",
        "    9: \"Retina 1\",\n",
        "    10: \"Retina 2\",\n",
        "    11: \"Retina 3\",\n",
        "    12: \"Retina 4\"\n",
        "}\n",
        "\n",
        "# Dataset configs\n",
        "DATASETS = {\n",
        "    \"octmnist\": (OCTMNIST, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (PneumoniaMNIST, [4, 5]),\n",
        "    \"breastmnist\": (BreastMNIST, [6, 7]),\n",
        "    \"retinamnist\": (RetinaMNIST, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "# Download and combine test data\n",
        "test_images_list = []\n",
        "test_labels_list = []\n",
        "\n",
        "for name, (dataset_class, label_offsets) in DATASETS.items():\n",
        "    # Download dataset\n",
        "    dataset = dataset_class(split='test', download=True, root=dataset_path)\n",
        "\n",
        "    for img, label in zip(dataset.imgs, dataset.labels):\n",
        "        img_pil = Image.fromarray(img.squeeze())  # Convert to grayscale if needed\n",
        "        img_tensor = transform(img_pil)\n",
        "        class_index = label_offsets[int(label[0])]\n",
        "        test_images_list.append(img_tensor.squeeze(0).numpy())  # remove channel dim\n",
        "        test_labels_list.append(class_index)\n",
        "\n",
        "# Convert lists to arrays\n",
        "test_images_array = np.array(test_images_list)\n",
        "test_labels_array = np.array(test_labels_list, dtype=np.int64)\n",
        "\n",
        "# Save combined test data\n",
        "combined_test_path = os.path.join(dataset_path, \"combined_test_dataset.npz\")\n",
        "np.savez_compressed(combined_test_path,\n",
        "                    test_images=test_images_array,\n",
        "                    test_labels=test_labels_array)\n",
        "\n",
        "print(f\"Combined test dataset saved at: {combined_test_path}\")\n",
        "print(f\"Total Test Samples: {len(test_images_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cFkqisqr0tXJ",
        "outputId": "9e629183-695a-4819-b40a-bb3211876c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined test dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/combined_test_dataset.npz\n",
            "Total Test Samples: 2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Test Data (Preprocessed)**"
      ],
      "metadata": {
        "id": "B07EMPLG8rNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from medmnist import OCTMNIST, PneumoniaMNIST, BreastMNIST, RetinaMNIST\n",
        "\n",
        "# Select your data path\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "\n",
        "# Define image transform (grayscale, tensor, normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Global label mapping\n",
        "global_label_mapping = {\n",
        "    0: \"choroidal neovascularization\",\n",
        "    1: \"diabetic macular edema\",\n",
        "    2: \"drusen\",\n",
        "    3: \"normal (OCT)\",\n",
        "    4: \"normal (Pneumonia)\",\n",
        "    5: \"pneumonia\",\n",
        "    6: \"malignant\",\n",
        "    7: \"normal, benign (Breast)\",\n",
        "    8: \"Retina 0\",\n",
        "    9: \"Retina 1\",\n",
        "    10: \"Retina 2\",\n",
        "    11: \"Retina 3\",\n",
        "    12: \"Retina 4\"\n",
        "}\n",
        "\n",
        "# Dataset configs\n",
        "DATASETS = {\n",
        "    \"octmnist\": (OCTMNIST, [0, 1, 2, 3]),\n",
        "    \"pneumoniamnist\": (PneumoniaMNIST, [4, 5]),\n",
        "    \"breastmnist\": (BreastMNIST, [6, 7]),\n",
        "    \"retinamnist\": (RetinaMNIST, [8, 9, 10, 11, 12])\n",
        "}\n",
        "\n",
        "def ensure_grayscale(img):\n",
        "    \"\"\"Ensure the image is 2D (grayscale).\"\"\"\n",
        "    if img.ndim == 3:\n",
        "        if img.shape[-1] == 1:\n",
        "            img = np.squeeze(img, axis=-1)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    return img\n",
        "\n",
        "# Apply same Gaussian blur for all datasets\n",
        "def apply_gaussian_blur(img):\n",
        "    img = ensure_grayscale(img).astype(np.uint8)\n",
        "    return cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "# Download and combine test data with preprocessing\n",
        "test_images_list = []\n",
        "test_labels_list = []\n",
        "\n",
        "for name, (dataset_class, label_offsets) in DATASETS.items():\n",
        "    dataset = dataset_class(split='test', download=True, root=dataset_path)\n",
        "\n",
        "    for img, label in zip(dataset.imgs, dataset.labels):\n",
        "        img = img.squeeze().astype(np.uint8)\n",
        "        enhanced_img = apply_gaussian_blur(img)\n",
        "        img_pil = Image.fromarray(enhanced_img)\n",
        "        img_tensor = transform(img_pil)\n",
        "\n",
        "        class_index = label_offsets[int(label[0])]\n",
        "        test_images_list.append(img_tensor.squeeze(0).numpy())\n",
        "        test_labels_list.append(class_index)\n",
        "\n",
        "# Convert to arrays\n",
        "test_images_array = np.array(test_images_list)\n",
        "test_labels_array = np.array(test_labels_list, dtype=np.int64)\n",
        "\n",
        "# Save preprocessed test set\n",
        "combined_test_path = os.path.join(dataset_path, \"pre_combined_test_dataset.npz\")\n",
        "np.savez_compressed(combined_test_path,\n",
        "                    test_images=test_images_array,\n",
        "                    test_labels=test_labels_array)\n",
        "\n",
        "print(f\"Preprocessed combined test dataset saved at: {combined_test_path}\")\n",
        "print(f\"Total Test Samples: {len(test_images_array)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xNOpz6Iq8wQP",
        "outputId": "5a5c7402-bc6d-4224-d114-b98aec77e3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed combined test dataset saved at: /content/drive/MyDrive/MIA_Project/dataset/pre_combined_test_dataset.npz\n",
            "Total Test Samples: 2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Models**"
      ],
      "metadata": {
        "id": "QLCyfpuVu9LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# Load the true test dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"combined_test_dataset.npz\"))\n",
        "\n",
        "test_images, test_labels = data[\"test_images\"], data[\"test_labels\"]\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Test loader\n",
        "batch_size = 64\n",
        "test_dataset = CustomDataset(test_images, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Modified ResNet18 model\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18()\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model_path, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ModifiedResNet18(num_classes=13).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovo\")\n",
        "\n",
        "    print(f\"\\nEvaluation Results: {model_name}\")\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"AUC      : {auc:.4f}\")\n",
        "\n",
        "# Paths\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "base_model_path = os.path.join(model_dir, \"resnet18_model.pth\")\n",
        "finetuned_model_path = os.path.join(model_dir, \"resnet18_model_finetuned.pth\")\n",
        "kfold_model_path = os.path.join(model_dir, \"resnet18_model_kfold_best.pth\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(base_model_path, \"Base Model (Trained Only)\")\n",
        "evaluate_model(finetuned_model_path, \"Fine-tuned Model (Train + Val)\")\n",
        "evaluate_model(kfold_model_path, \"Final Model (K-Fold + Combined Dataset)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvwvVUd8vBUU",
        "outputId": "75ed9002-89e2-421b-888b-7fc7c1641bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Base Model (Trained Only)\n",
            "Accuracy : 0.7216\n",
            "Precision: 0.7015\n",
            "Recall   : 0.7216\n",
            "AUC      : 0.9430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Fine-tuned Model (Train + Val)\n",
            "Accuracy : 0.7193\n",
            "Precision: 0.7337\n",
            "Recall   : 0.7193\n",
            "AUC      : 0.9404\n",
            "\n",
            "Evaluation Results: Final Model (K-Fold + Combined Dataset)\n",
            "Accuracy : 0.7394\n",
            "Precision: 0.7281\n",
            "Recall   : 0.7394\n",
            "AUC      : 0.9395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Models (Preprocessed)**"
      ],
      "metadata": {
        "id": "TK0S-HCIDErD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "# Load the true test dataset\n",
        "project_dir = \"/content/drive/MyDrive/MIA_Project\"\n",
        "dataset_path = os.path.join(project_dir, \"dataset\")\n",
        "data = np.load(os.path.join(dataset_path, \"pre_combined_test_dataset.npz\"))\n",
        "\n",
        "pre_test_images, pre_test_labels = data[\"test_images\"], data[\"test_labels\"]\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        return img, label\n",
        "\n",
        "# Test loader\n",
        "batch_size = 64\n",
        "pre_test_dataset = CustomDataset(pre_test_images, pre_test_labels)\n",
        "pre_test_loader = DataLoader(pre_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Modified ResNet18 model\n",
        "class ModifiedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(ModifiedResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18()\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model_path, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ModifiedResNet18(num_classes=13).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pre_test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    pre_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    pre_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    pre_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    pre_auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovo\")\n",
        "\n",
        "    print(f\"\\nEvaluation Results: {model_name}\")\n",
        "    print(f\"Accuracy : {pre_accuracy:.4f}\")\n",
        "    print(f\"Precision: {pre_precision:.4f}\")\n",
        "    print(f\"Recall   : {pre_recall:.4f}\")\n",
        "    print(f\"AUC      : {pre_auc:.4f}\")\n",
        "\n",
        "# Paths\n",
        "model_dir = os.path.join(project_dir, \"model\")\n",
        "base_model_path = os.path.join(model_dir, \"resnet18_preprocessed_model.pth\")\n",
        "finetuned_model_path = os.path.join(model_dir, \"resnet18_preprocessed_model_finetuned.pth\")\n",
        "kfold_model_path = os.path.join(model_dir, \"resnet18_preprocessed_model_kfold_best.pth\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(base_model_path, \"Base Model (Trained Only - Preprocessed)\")\n",
        "evaluate_model(finetuned_model_path, \"Fine-tuned Model (Train + Val - Preprocessed)\")\n",
        "evaluate_model(kfold_model_path, \"Final Model (K-Fold + Combined Dataset - Preprocessed)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "040eafb3-087d-4d37-ffc2-2bc2051873c4",
        "id": "hKZTib0THLiw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Base Model (Trained Only - Preprocessed)\n",
            "Accuracy : 0.6986\n",
            "Precision: 0.7086\n",
            "Recall   : 0.6986\n",
            "AUC      : 0.9410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results: Fine-tuned Model (Train + Val - Preprocessed)\n",
            "Accuracy : 0.6853\n",
            "Precision: 0.7205\n",
            "Recall   : 0.6853\n",
            "AUC      : 0.9431\n",
            "\n",
            "Evaluation Results: Final Model (K-Fold + Combined Dataset - Preprocessed)\n",
            "Accuracy : 0.6647\n",
            "Precision: 0.6955\n",
            "Recall   : 0.6647\n",
            "AUC      : 0.9405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}